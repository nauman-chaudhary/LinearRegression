{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setting_data(data=None):\n",
    "    feature_matrix = np.ones((np.ma.size(data,axis=0),np.ma.size(data,axis=1)+1))\n",
    "    feature_matrix[:,1:] = data\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_values(feature_matrix=None, weights=None):\n",
    "    return np.dot(a=feature_matrix,b=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features_generation(data=None, degree=0):\n",
    "    return np.hstack((data,np.power(data,np.arange(start=2,stop=degree+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(data=None):\n",
    "    data = data.astype(float)\n",
    "    for i in range(data.shape[1]):\n",
    "        data[:,i] = np.divide(np.subtract(data[:,i],np.mean(data[:,i])), np.std(data[:,i])).astype(float)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(data=None):\n",
    "    data = data.astype(float)\n",
    "    for i in range(data.shape[1]):\n",
    "        min_ = np.min(data[:,i])\n",
    "        max_ = np.max(data[:,i])\n",
    "        max_minus_min = max_ - min_\n",
    "        data[:,i] = np.divide(np.subtract(data[:,i],min_),max_minus_min)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(feature_matrix=None):\n",
    "    norms = np.sqrt(np.sum(np.square(feature_matrix),axis=0))\n",
    "    normalized_features = feature_matrix/norms\n",
    "    return (normalized_features,norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y=None,y_bar=None,no_of_observations=0):\n",
    "    return np.sqrt(np.divide(np.sum(np.square(np.subtract(y,y_bar))),no_of_observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x=None,y=None,tolerance_gradient=0.01, tolerance_cost=0.0000001,\n",
    "                     alpha = 0.001, max_iter=10000,l2_regularization=0):\n",
    "   \n",
    "    feature_matrix = setting_data(data=x)\n",
    "    \n",
    "    weights = np.random.rand(x.shape[1] + 1, 1)\n",
    "    partials = np.zeros(x.shape[1]+1).reshape((-1,1))\n",
    "    \n",
    "    i = 0\n",
    "    prev_cost = 10\n",
    "    costs = []\n",
    "    gradients = []\n",
    "    \n",
    "    converged = False\n",
    "    while not converged:\n",
    "        i += 1\n",
    "        \n",
    "        predicted = predict_values(feature_matrix=feature_matrix, weights=weights)\n",
    "        errors = np.subtract(predicted, y)\n",
    "        #Updating weights\n",
    "        \n",
    "        #taking derivative\n",
    "        partials = np.dot(feature_matrix.T,errors)\n",
    "        #This is the total change :p\n",
    "#         step_size = np.multiply(alpha,partials)\n",
    "#         weights_regularized = np.vstack((weights[0],np.multiply(weights[1:],1-2*alpha*l2_regularization)))\n",
    "        \n",
    "        weights = np.subtract(np.vstack((weights[0],np.multiply(weights[1:],1-2*alpha*l2_regularization))),\n",
    "                              np.multiply(alpha,partials))\n",
    "        \n",
    "        gradient_magnitude = np.sqrt(np.sum(np.square(partials)))\n",
    "        cur_cost = np.sum(np.square(errors))\n",
    "        \n",
    "#         gradients = np.append(gradients,gradient_magnitude)\n",
    "#         costs = np.append(costs,cur_cost)\n",
    "        gradients.append(gradient_magnitude)\n",
    "        costs.append(cur_cost)\n",
    "\n",
    "#         if i%1000==0:\n",
    "#             print(cur_cost)\n",
    "#         print(abs(cur_cost - prev_cost))\n",
    "        if (gradient_magnitude <= tolerance_gradient) or (abs(cur_cost - prev_cost) < tolerance_cost):\n",
    "            if gradient_magnitude <= tolerance_gradient:\n",
    "                print('[Gradient Magnitude - Tolerance] Satisfied')\n",
    "            else:\n",
    "                print('[Cost Tolerance - Precision] Satisfied')\n",
    "            converged = True\n",
    "\n",
    "        if i >= max_iter:\n",
    "            print('Iterations Completed')\n",
    "            converged = True\n",
    "        \n",
    "        prev_cost = cur_cost\n",
    "    return (i, np.array(weights),np.array(gradients), costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.insert(weights,j,weight_j,0)def coordinate_descent(x=None,y=None,tolerance_gradient=0.01, \n",
    "                       tolerance_cost=0.0000001,alpha = 0.001, max_iter=10000,\n",
    "                       l1_regularization=0):\n",
    "    \n",
    "    feature_matrix = setting_data(data=x)\n",
    "    \n",
    "    weights = np.random.rand(x.shape[1] + 1, 1)\n",
    "    iteration = 0\n",
    "    prev_cost = 10\n",
    "    costs = []\n",
    "    gradients = []\n",
    "    \n",
    "    converged = False\n",
    "    while not converged:\n",
    "        iteration += 1\n",
    "        for j in range(0,x.shape[1]-1):\n",
    "            #saving for later insertion\n",
    "            feature_j = feature_matrix[:,j]\n",
    "            weight_j = weights[j]\n",
    "            \n",
    "            #deleting from matrix for performing operation on matrix\n",
    "            feature_matrix = np.delete(feature_matrix,j,1)\n",
    "            weights = np.delete(weights,j,0)\n",
    "            \n",
    "            #performing operations\n",
    "            predicted = predict_values(feature_matrix=feature_matrix, weights=weights)\n",
    "            \n",
    "            #Inserting values back in matrices\n",
    "            feature_matrix = np.insert(feature_matrix,j,feature_j,1)\n",
    "            weights = np.insert(weights,j,weight_j,0)\n",
    "            \n",
    "            #calculating errors\n",
    "            errors = np.subtract(predicted, y)\n",
    "            \n",
    "            roo_j = np.dot(feature_matrix.T,errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_types = dtype_dict = {'bathrooms':float, 'waterfront':int, \n",
    "                           'sqft_above':int, 'sqft_living15':float, \n",
    "                           'grade':int, 'yr_renovated':int, 'price':float, \n",
    "                           'bedrooms':float, 'zipcode':str, 'long':float, \n",
    "                           'sqft_lot15':float, 'sqft_living':float, \n",
    "                           'floors':str, 'condition':int, 'lat':float, \n",
    "                           'date':str, 'sqft_basement':int, 'yr_built':int, \n",
    "                           'id':str, 'sqft_lot':int, 'view':int}\n",
    "data= pd.read_csv('Related Datasets/kc_house_train_data.csv',dtype=data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19950585, 0.39435213, 0.92667488, 0.90217904, 0.33609493],\n",
       "       [0.2168195 , 0.78884719, 0.57322897, 0.33414328, 0.48069736],\n",
       "       [0.62707871, 0.95453995, 0.72615349, 0.94609055, 0.84480906],\n",
       "       [0.81113138, 0.83506624, 0.99519287, 0.37987991, 0.61312186],\n",
       "       [0.82230393, 0.1161766 , 0.74884054, 0.61704703, 0.64625948],\n",
       "       [0.30762416, 0.09954673, 0.50343005, 0.48840257, 0.46650352],\n",
       "       [0.01957862, 0.86787931, 0.50425555, 0.4775349 , 0.87098522],\n",
       "       [0.95503305, 0.90424297, 0.95294636, 0.42927533, 0.74250794],\n",
       "       [0.72229204, 0.16221511, 0.05803657, 0.97092983, 0.25494678],\n",
       "       [0.0511185 , 0.80221669, 0.27413082, 0.00728241, 0.94093357]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(50).reshape(10,5)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92667488],\n",
       "       [0.57322897],\n",
       "       [0.72615349],\n",
       "       [0.99519287],\n",
       "       [0.74884054],\n",
       "       [0.50343005],\n",
       "       [0.50425555],\n",
       "       [0.95294636],\n",
       "       [0.05803657],\n",
       "       [0.27413082]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X[:,2]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = X[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19950585, 0.39435213, 0.90217904, 0.33609493],\n",
       "       [0.2168195 , 0.78884719, 0.33414328, 0.48069736],\n",
       "       [0.62707871, 0.95453995, 0.94609055, 0.84480906],\n",
       "       [0.81113138, 0.83506624, 0.37987991, 0.61312186],\n",
       "       [0.82230393, 0.1161766 , 0.61704703, 0.64625948],\n",
       "       [0.30762416, 0.09954673, 0.48840257, 0.46650352],\n",
       "       [0.01957862, 0.86787931, 0.4775349 , 0.87098522],\n",
       "       [0.95503305, 0.90424297, 0.42927533, 0.74250794],\n",
       "       [0.72229204, 0.16221511, 0.97092983, 0.25494678],\n",
       "       [0.0511185 , 0.80221669, 0.00728241, 0.94093357]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X = np.delete(X,2,1)\n",
    "new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.99505847e-01, 3.94352128e-01, 1.00000000e+00, 9.26674884e-01,\n",
       "        9.02179043e-01, 3.36094932e-01],\n",
       "       [2.16819501e-01, 7.88847194e-01, 2.00000000e+00, 5.73228973e-01,\n",
       "        3.34143281e-01, 4.80697365e-01],\n",
       "       [6.27078709e-01, 9.54539950e-01, 3.00000000e+00, 7.26153493e-01,\n",
       "        9.46090549e-01, 8.44809063e-01],\n",
       "       [8.11131377e-01, 8.35066238e-01, 4.00000000e+00, 9.95192874e-01,\n",
       "        3.79879910e-01, 6.13121857e-01],\n",
       "       [8.22303933e-01, 1.16176605e-01, 5.00000000e+00, 7.48840540e-01,\n",
       "        6.17047035e-01, 6.46259477e-01],\n",
       "       [3.07624158e-01, 9.95467296e-02, 6.00000000e+00, 5.03430053e-01,\n",
       "        4.88402565e-01, 4.66503521e-01],\n",
       "       [1.95786178e-02, 8.67879310e-01, 7.00000000e+00, 5.04255548e-01,\n",
       "        4.77534903e-01, 8.70985219e-01],\n",
       "       [9.55033054e-01, 9.04242969e-01, 8.00000000e+00, 9.52946361e-01,\n",
       "        4.29275326e-01, 7.42507945e-01],\n",
       "       [7.22292042e-01, 1.62215106e-01, 9.00000000e+00, 5.80365655e-02,\n",
       "        9.70929831e-01, 2.54946778e-01],\n",
       "       [5.11184999e-02, 8.02216693e-01, 1.00000000e+01, 2.74130823e-01,\n",
       "        7.28240724e-03, 9.40933571e-01]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.insert(X,2,np.array([1,2,3,4,5,6,7,8,9,10]),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.random.rand(10+ 1, 1)\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82059623],\n",
       "       [0.62153869],\n",
       "       [0.47531738],\n",
       "       [0.0678297 ],\n",
       "       [0.10293724],\n",
       "       [0.38557779],\n",
       "       [0.55093306],\n",
       "       [0.82720093],\n",
       "       [0.47093431],\n",
       "       [0.11559756],\n",
       "       [0.05300184]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82059623],\n",
       "       [0.47531738],\n",
       "       [0.0678297 ],\n",
       "       [0.10293724],\n",
       "       [0.38557779],\n",
       "       [0.55093306],\n",
       "       [0.82720093],\n",
       "       [0.47093431],\n",
       "       [0.11559756],\n",
       "       [0.05300184]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.delete(weights,1,0)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.20596230e-01],\n",
       "       [1.01000000e+02],\n",
       "       [4.75317381e-01],\n",
       "       [6.78296987e-02],\n",
       "       [1.02937240e-01],\n",
       "       [3.85577791e-01],\n",
       "       [5.50933059e-01],\n",
       "       [8.27200934e-01],\n",
       "       [4.70934308e-01],\n",
       "       [1.15597556e-01],\n",
       "       [5.30018394e-02]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.insert(weights,1,1,0)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
